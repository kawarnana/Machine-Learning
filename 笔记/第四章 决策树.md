## 1. 基本流程
### 1.1 决策树(Decision tree)
       决策树(decision tree)是一种基本的分类与回归方法.   
       在分类问题中，表示基于属性(或特征)对样本(或实例)进行分类的过程。
       
决策树学习的目的：为了产生一颗泛化能力强，即处理未见示例能力强的决策树。

决策树由结点(node)和有向边(directed edge)组成。   
      结点有：内部结点(internal node)---表示一个属性(或特征)，叶节点(leaf node)---表示一种决策结果(或类)
      
### 1.2 决策树与 if-then路径
决策树可看成是一个if-then路径的集合，**决策树的路径有一个重要的性质：互斥并且完备。** ，即每一个样本都被一条路径所覆盖，而且只被一条路径所覆盖。
![Alt text](https://github.com/kawarnana/Machine-Learning/blob/master/pictures/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95.png)
    
    在决策树基本算法中，有三种情况会导致递归返回：
    case1: 当前节点包含的样本全属于同一类别，无需划分。   
    case2: 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分。
    case3: 当前节点包含的样本集合为空，不能划分。
    
## 2. 划分选择

随着划分过程的不断进行，我们希望决策树的分支节点包含的样本尽可能属于同一类别，即结点的“纯度”(purity)越来越高。
![Alt text](https://github.com/kawarnana/Machine-Learning/blob/master/pictures/%E4%BF%A1%E6%81%AF%E7%86%B5.PNG)

### 2.1 信息增益(information entropy)

![Alt text](https://github.com/kawarnana/Machine-Learning/blob/master/pictures/%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A.PNG) 

信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好带来的不利影响  
**著名C4.5算法直接使用信息增益，而使用“增益率(gain ratio)来选择最优划分属性。”**

### 2.2 增益率(gain ratio)

![Alt text](https://github.com/kawarnana/Machine-Learning/blob/master/pictures/%E5%A2%9E%E7%9B%8A%E7%8E%87.PNG)   
增益率准则对可取值数目较少的属性有所偏好，   
**C4.5并不是直接选择增益率最大的候选划分属性**   
**而是先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。**

### 2.3 基尼指数

![Alt text](https://github.com/kawarnana/Machine-Learning/blob/master/pictures/%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A.PNG)

## 3. 剪枝处理(Pruning)

### 剪枝:决策树学习算法处理“过拟合”的主要手段
      为尽可能正确分类训练样本，有时会造成决策树分支过多(学得太好，训练集自身一些特点被当做一般属性导致过拟合)   
  因此，可以通过去掉一些分支来降低过拟合的风险。

     基本策略：预剪枝(prepruning)：   
                    在决策树生成过程中，对每个节点在划分钱先进行估计，若当前节点的划分不能带来决策树泛化能力的提升，则停止划分并将当前节点标记为叶结点。  
            后剪枝(postpruning)
## 4. 连续值与缺失值
## 5. 多变量决策树
