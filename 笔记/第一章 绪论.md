## 1.根据训练数据是否拥有标记信息，学习任务可以大致划分为两类：
     监督学习（supervised learning ）： 分类，回归  
     非监督学习(unsupervised learning): 聚类

## 2.机器学习的目标：
  使学得的模型能很好地适用于“新样本”，不仅仅是在训练样本上工作的很好。 
     学得模型适用于新样本的能力称作“泛化”。 
   
## 3.版本空间 
 现实问题中市场面临很大的假设空间，但学习过程是基于有限样本训练集进行的。
 故可能有多个假设与训练集一致，即存在一个与训练集一致的“假设集合”，称为“版本空间”。
 
 ## 4.归纳偏好
 通过学习得到的模型对应了假设空间中的一个假设。 
 版本空间带来问题： 有n个与训练集一致的假设，但与他们对应的模型在面临新样本时，可以产生不同的输出。 
该采用哪种模型就需要参考归纳偏好来决定.
   
     若认为相似的样本应有相似的输出（比如，各种属性上都很相像的西瓜，成熟度应该比较接近），
     则对应的学习算法可能偏好图中比较“平滑”的曲线A，而非较为“崎岖”的曲线B。
  ![Image text](https://github.com/kawarnana/Machine-Learning/blob/master/pictures/%E5%BD%92%E7%BA%B3%E5%81%8F%E5%A5%BD.PNG)
 
 ## 5.“没有免费午餐”定理
  无论学习算法多么聪明，或者多么笨拙，其期望性能一致。
  
    前提：所有“问题”出现的机会相同，或者所有问题同等重要。
 ![Image text](https://github.com/kawarnana/Machine-Learning/blob/master/pictures/%E5%A4%A9%E4%B8%8B%E6%B2%A1%E6%9C%89%E5%85%8D%E8%B4%B9%E7%9A%84%E5%8D%88%E9%A4%90.PNG)
             
     寓意：脱离具体问题，空泛的谈论“什么学习算法更好”毫无意义。
 
