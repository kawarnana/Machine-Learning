## 3.1 线性模型 (linear model)
试图学到一个通过属性的线性组合来进行预测的函数
![Alt text](https://github.com/kawarnana/Machine-Learning/blob/master/pictures/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%90%91%E9%87%8F%E5%BD%A2%E5%BC%8F.PNG)

## 3.2 线性回归(linear regeression)
#### 1. 最小二乘法(least square method)
基于均方误差最小化来进行模型求解的方法。  
在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。
#### 2. 多元线性回归(multivariate linear regression)
## 3.3 对数几率回归(logistic regression)
### 虽然名字叫做“回归”，但却是一种分类学习方法。
    优点 1.直接对分类可能性进行建模，无需事先假设数据分布，避免了假设分布不准确所带来的问题。  
	     2.并非预测出类别，而是可得到近似概率预测，对许多需利用概率辅助决策的任务有用。
         3.求解的目标函数是任意阶可导的凸函数，有很好的的数学行知，许多数值优化算法都可以直接用于求取最优解。
考虑二分类任务，输出标记y∈{0,1},而现行回归模型产生的预测值是实值，于是需要将实值转换为0/1值。  
最理想的是“单位阶跃函数”，可惜单位阶跃函数不连续。由此引出对数几率函数作为替代函数。![Alt text](https://github.com/kawarnana/Machine-Learning/blob/master/pictures/%E5%AF%B9%E6%95%B0%E5%87%A0%E7%8E%87%E5%87%BD%E6%95%B0%E4%B8%8E%E5%8D%95%E4%BD%8D%E9%98%B6%E8%B7%83%E5%87%BD%E6%95%B0.PNG)

## 3.4 线性判别分析(Linear Discriminant Analysis,LDA)
     思想：给定训练样例集，设法将样例投影到一条直线上，  
          使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离。  
          在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别。
![Alt text](https://github.com/kawarnana/Machine-Learning/blob/master/pictures/LDA.PNG)
## 3.5 多分类学习最经典的拆分策略有三种：  
          “一对一”(One vs. One,OvO)  
          “一对其余”(One vs. Rest,OvR)  
          “多对多”(Many vs. Many,MvM): 常用纠错输出码(Error Correcting Output Codes,ECOC)，其对分类器的错误有一定的容忍和修正能力。

                                       汉明距离：码距信息编码中，两合法代码对应位上编码数不同的位数。
## 3.6 类别不平衡问题(class-imbalance)
1.类别不平衡：指分类任务中不同类别的训练样例数目差别很大的情况。
2.策略：“再缩放”(rescaling)
3.技术大体做法：
     第一类：欠采样(undersampling),即去除一些反例使得正、反例数目接近，然后再进行学习。
	 第二类：过采样(oversampling),即增加一些正例使得正、反例数目接近，然后再进行学习。
	 第三类：阈值移动


## 这周考试时间有些紧张，笔记内容自己不是很满意，缺少数学推导，过了考试周再补一下。
